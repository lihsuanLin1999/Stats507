{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bab5130",
   "metadata": {},
   "source": [
    "# Stats507 HW2\n",
    "\n",
    "# Name: Li-Hsuan Lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6074cc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import statistics\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d86ff5",
   "metadata": {},
   "source": [
    "# Question 0 - Code review warmup"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad1c34c5",
   "metadata": {},
   "source": [
    "sample_list = [(1, 3, 5), (0, 1, 2), (1, 9, 8)]\n",
    "op = []\n",
    "\n",
    "for m in range(len(sample_list)):\n",
    "    li = [sample_list[m]]\n",
    "        for n in range(len(sample_list)):\n",
    "            if (sample_list[m][0] == sample_list[n][0] and\n",
    "                    sample_list[m][3] != sample_list[n][3]):\n",
    "                li.append(sample_list[n])\n",
    "        op.append(sorted(li, key=lambda dd: dd[3], reverse=True)[0])\n",
    "res = list(set(op))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d98dbd",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16326e6",
   "metadata": {},
   "source": [
    "The snippet of code take a list of tuples and return a list of distinct tuples of which, considering the tuples with the first same element, the tuple with maximum third element would be contained in the returned list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bcfaa0",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51173a27",
   "metadata": {},
   "source": [
    "There are a couple ways to make the snippet of code more readable and pythonic.\n",
    "\n",
    "1. Instead of using \"for m in range(len(sample_list))\", it is more pythonic to use \"for m in sample_list\" (iterate thru item in a list rather than thru index and then use index to access item)\n",
    "\n",
    "2. The indentation is incorrect. The entire second for loop should have the same indentation as the \"li = [sample_list[m]]\"\n",
    "3. In sorted function, maybe it's good idea to use more readable lambda variable instead of \"dd\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c01d6e",
   "metadata": {},
   "source": [
    "# Question 1 - List of Tuples"
   ]
  },
  {
   "cell_type": "raw",
   "id": "602cd500",
   "metadata": {},
   "source": [
    "Write a function that uses NumPy and a list comprehension to generate a random list of n k-tuples \n",
    "containing integers ranging from low to high. Choose an appropriate name for your function, \n",
    "and reasonable default values for k, low, and high.\n",
    "\n",
    "Use assert to test that your function returns a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfeb0092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randList_generator(n, k=3, low=0, high=20):\n",
    "    \"\"\"uses NumPy and a list comprehension to generate a random list of n k-tuples \n",
    "       containing integers ranging from low to high\"\"\"\n",
    "    randList = [tuple(sorted(np.random.choice(range(low,high), size=k,replace=False))) for _ in range(n)]\n",
    "    return (randList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39fc0d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "randList = randList_generator(5, k=3, low=0, high=12)\n",
    "\n",
    "# assert it is a list\n",
    "assert(isinstance(randList, list))\n",
    "\n",
    "# assert list contains only tuples\n",
    "for item in randList:\n",
    "    assert(isinstance(item, tuple))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99cd8b",
   "metadata": {},
   "source": [
    "# Question 2 - Refactor the Snippet"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35de41ee",
   "metadata": {},
   "source": [
    "Encapsulate the code snippet from the warmup into a function that parameterizes the role of 0 and 3 and is otherwise unchanged. Choose appropriate names for these paramters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121cf318",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "304dd27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = [(1, 3, 5), (0, 1, 2), (1, 9, 8)]\n",
    "op = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "979073d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use first index and second index to parameterizes\n",
    "\n",
    "def refactor_partA(sample_list, first_idx, second_idx):\n",
    "    \"\"\"parameterizes the role of 0 and 3 in snippert and otherwise unchanged\"\"\"\n",
    "    op = []\n",
    "    for m in range(len(sample_list)):\n",
    "        li = [sample_list[m]]\n",
    "        for n in range(len(sample_list)):\n",
    "            if (sample_list[m][first_idx] == sample_list[n][first_idx] and \n",
    "                sample_list[m][second_idx] != sample_list[n][second_idx]):\n",
    "                li.append(sample_list[n])\n",
    "        op.append(sorted(li, key=lambda dd: dd[second_idx], reverse=True)[0])\n",
    "    res = list(set(op))\n",
    "    return (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5f1c55",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0068797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refactor_partB(sample_list, first_idx, second_idx):\n",
    "    \"\"\"parameterizes the role of 0 and 3 in snippert and use code review suggestion\"\"\"\n",
    "    op = []\n",
    "    for m in sample_list:\n",
    "        li = [m]\n",
    "        for n in sample_list:\n",
    "            if m[first_idx] == n[first_idx] and m[second_idx] != n[second_idx]:\n",
    "                li.append(n)\n",
    "        op.append(sorted(li, key=lambda item: item[second_idx], reverse=True)[0])\n",
    "    res = list(set(op))\n",
    "    return (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f90486",
   "metadata": {},
   "source": [
    "## same result from partA and partB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30e96dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = [(1, 3, 5), (0, 1, 2), (1, 9, 8),(1,2,5),(0,1,5),(3,6,8),(0,1,7)]\n",
    "assert(refactor_partA(sample_list,0, 2) == refactor_partB(sample_list,0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8030a08",
   "metadata": {},
   "source": [
    "## C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae38e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refactor_partC(sample_lst, first_idx, second_idx):\n",
    "    \"\"\"Same function as the snippet of code above. \n",
    "       traverse the input list of tuples no more than twice.\"\"\"\n",
    "    \n",
    "    refactor_dict = {} # key: first elem, value: a tuple\n",
    "    se = set()\n",
    "    \n",
    "    # traverse once \n",
    "    for item in sample_lst:\n",
    "        curr_first_elem = item[first_idx]\n",
    "        curr_second_elem = item[second_idx]\n",
    "        \n",
    "        # if the dictionary is empty\n",
    "        if curr_first_elem not in refactor_dict:\n",
    "            refactor_dict[curr_first_elem] = item\n",
    "        else:\n",
    "            # if the second element is greater than that stored in the dictionary\n",
    "            if curr_second_elem > refactor_dict[curr_first_elem][second_idx]:\n",
    "                refactor_dict[curr_first_elem] = item\n",
    "                \n",
    "    # traverse list again to replace list item            \n",
    "    for item in sample_lst:\n",
    "        first_elem = item[first_idx]\n",
    "        item_max = refactor_dict[first_elem]\n",
    "        \n",
    "        se.add(item_max)\n",
    "    \n",
    "    li = list(se)\n",
    "        \n",
    "    return li"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ea197",
   "metadata": {},
   "source": [
    "# d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fea169",
   "metadata": {},
   "source": [
    "I use 100 replications to run functions A, functions B, functions C. In each replication, a list with 30 tuples with length 3 is generated, and each function (A, B, C) is ran 1000 times. The mean of the runtime is given in the dataframe in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c7d1a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_a, runtime_b, runtime_c = [],[],[]\n",
    "\n",
    "for _ in range(100):\n",
    "    randLst = randList_generator(30, k=3, low=0, high=40)\n",
    "    \n",
    "    funcA_time = timeit.timeit(lambda: refactor_partA(randLst,0,2),number = 1000)\n",
    "    funcB_time = timeit.timeit(lambda: refactor_partB(randLst,0,2),number = 1000)\n",
    "    funcC_time = timeit.timeit(lambda: refactor_partC(randLst,0,2),number = 1000)\n",
    "    \n",
    "    runtime_a.append(funcA_time)\n",
    "    runtime_b.append(funcB_time)\n",
    "    runtime_c.append(funcC_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b132ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_runtimeA = sum(runtime_a)/len(runtime_a)\n",
    "mean_runtimeB = sum(runtime_b)/len(runtime_b)\n",
    "mean_runtimeC = sum(runtime_c)/len(runtime_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9882a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>function A</th>\n",
       "      <td>0.272663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>function B</th>\n",
       "      <td>0.198917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>function C</th>\n",
       "      <td>0.019943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean runtime\n",
       "function A      0.272663\n",
       "function B      0.198917\n",
       "function C      0.019943"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'mean runtime': [mean_runtimeA, mean_runtimeB, mean_runtimeC]}\n",
    "df = pd.DataFrame(data=d,index=['function A','function B', 'function C'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfbef5e",
   "metadata": {},
   "source": [
    "## Use assert to test three functions have the same output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b49b5684",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = [(1, 3, 5), (0, 1, 2), (1, 9, 8),(1,2,5),(0,1,5),(3,6,8),(0,1,7)]\n",
    "assert(refactor_partA(sample_list,0, 2) == refactor_partB(sample_list,0, 2) == refactor_partC(sample_list,0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98f200a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "randLst_1 = randList_generator(10, k=3, low=0, high=40)\n",
    "randLst_2 = randList_generator(20, k=3, low=0, high=40)\n",
    "\n",
    "\n",
    "assert(refactor_partA(randLst_1,0, 2) == refactor_partB(randLst_1,0, 2) == refactor_partC(randLst_1,0, 2))\n",
    "assert(refactor_partA(randLst_2,0, 2) == refactor_partB(randLst_2,0, 2) == refactor_partC(randLst_2,0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c41a1a",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a885a8",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbe5c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets\n",
    "dat_g = pd.read_sas(\"DEMO_G.XPT\")\n",
    "dat_h = pd.read_sas(\"DEMO_H.XPT\")\n",
    "dat_i = pd.read_sas(\"DEMO_I.XPT\")\n",
    "dat_j = pd.read_sas(\"DEMO_J.XPT\")\n",
    "\n",
    "# Add an additional column identifying to which cohort each case belongs\n",
    "dat_g['cohort'] = '11-12'\n",
    "dat_h['cohort'] = '13-14'\n",
    "dat_i['cohort'] = '15-16'\n",
    "dat_j['cohort'] = '17-18'\n",
    "\n",
    "# append multiple dataframes to one\n",
    "frames = [dat_g, dat_h,dat_i,dat_j]\n",
    "pd_all = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eafc2315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset columns\n",
    "df_all = pd_all[[\"cohort\",\"SEQN\",\"RIDAGEYR\",\"RIDRETH3\",\"DMDEDUC2\",\n",
    "               \"DMDMARTL\",\"RIDSTATR\",\"SDMVPSU\",\"SDMVSTRA\",\"WTMEC2YR\",\n",
    "               \"WTINT2YR\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dc32620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off pandas chained assignment\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "# create a list that has the categorical datatype columns\n",
    "categpry_cols = ['cohort','SEQN','RIDRETH3','DMDEDUC2','DMDMARTL']\n",
    "\n",
    "# create a list that has the integer datatype columns\n",
    "integer_cols = ['RIDAGEYR','RIDSTATR','SDMVPSU','SDMVSTRA']\n",
    "\n",
    "# use for loops tp convert the data to category type and int32 type\n",
    "for col in categpry_cols:\n",
    "    df_all[col] = df_all[col].astype('category')\n",
    "for col in integer_cols:\n",
    "    df_all[col] = df_all[col].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37cd1527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cohort      category\n",
       "SEQN        category\n",
       "RIDAGEYR       int64\n",
       "RIDRETH3    category\n",
       "DMDEDUC2    category\n",
       "DMDMARTL    category\n",
       "RIDSTATR       int64\n",
       "SDMVPSU        int64\n",
       "SDMVSTRA       int64\n",
       "WTMEC2YR     float64\n",
       "WTINT2YR     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display dataframe datatypes\n",
    "df_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1eb130d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns to literal name\n",
    "df_all.rename(columns={'SEQN': 'id', \n",
    "                   'RIDAGEYR': 'age',\n",
    "                   'RIDRETH3':'race and ethnicity',\n",
    "                   'DMDEDUC2':'education',\n",
    "                   'DMDMARTL':'marital status'\n",
    "                    }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a87c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to pickle format\n",
    "df_all.to_pickle(\"nhanes.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8c50ec",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd81102",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\l6040\\anaconda3\\envs\\stats\\lib\\site-packages\\pandas\\io\\sas\\sas_xport.py:475: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[x] = v\n",
      "C:\\Users\\l6040\\AppData\\Local\\Temp/ipykernel_17716/2382609499.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dat_teeth_g['cohort'] = '11-12'\n",
      "C:\\Users\\l6040\\AppData\\Local\\Temp/ipykernel_17716/2382609499.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dat_teeth_h['cohort'] = '13-14'\n",
      "C:\\Users\\l6040\\AppData\\Local\\Temp/ipykernel_17716/2382609499.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dat_teeth_i['cohort'] = '15-16'\n",
      "C:\\Users\\l6040\\AppData\\Local\\Temp/ipykernel_17716/2382609499.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dat_teeth_j['cohort'] = '17-18'\n"
     ]
    }
   ],
   "source": [
    "# load the datasets\n",
    "dat_teeth_g = pd.read_sas(\"OHXDEN_G.XPT\")\n",
    "dat_teeth_h = pd.read_sas(\"OHXDEN_H.XPT\")\n",
    "dat_teeth_i = pd.read_sas(\"OHXDEN_I.XPT\")\n",
    "dat_teeth_j = pd.read_sas(\"OHXDEN_J.XPT\")\n",
    "\n",
    "# create a column to identify to which cohort each case belongs.\n",
    "dat_teeth_g['cohort'] = '11-12'\n",
    "dat_teeth_h['cohort'] = '13-14'\n",
    "dat_teeth_i['cohort'] = '15-16'\n",
    "dat_teeth_j['cohort'] = '17-18'\n",
    "\n",
    "# concatenate 4 dataframes to one dataframe\n",
    "frames = [dat_teeth_g, dat_teeth_h,dat_teeth_i,dat_teeth_j]\n",
    "pd_teeth_all = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69c68e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to pickle format\n",
    "pd_teeth_all.to_pickle(\"nhanes_teeth.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1667b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>OHDEXSTS</th>\n",
       "      <th>OHDDESTS</th>\n",
       "      <th>OHXIMP</th>\n",
       "      <th>OHX01TC</th>\n",
       "      <th>OHX02TC</th>\n",
       "      <th>OHX03TC</th>\n",
       "      <th>OHX04TC</th>\n",
       "      <th>OHX05TC</th>\n",
       "      <th>OHX06TC</th>\n",
       "      <th>...</th>\n",
       "      <th>OHX26RSC</th>\n",
       "      <th>OHX27RSC</th>\n",
       "      <th>OHX28RSC</th>\n",
       "      <th>OHX29RSC</th>\n",
       "      <th>OHX30RSC</th>\n",
       "      <th>OHX31RSC</th>\n",
       "      <th>OHXRCAR</th>\n",
       "      <th>OHXRCARO</th>\n",
       "      <th>OHXRRES</th>\n",
       "      <th>OHXRRESO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8365</th>\n",
       "      <td>102956.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>b''</td>\n",
       "      <td>b''</td>\n",
       "      <td>b''</td>\n",
       "      <td>b''</td>\n",
       "      <td>b''</td>\n",
       "      <td>b''</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          SEQN  OHDEXSTS  OHDDESTS  OHXIMP  OHX01TC  OHX02TC  OHX03TC  \\\n",
       "8365  102956.0       1.0       1.0     2.0      4.0      4.0      2.0   \n",
       "\n",
       "      OHX04TC  OHX05TC  OHX06TC  ...  OHX26RSC  OHX27RSC  OHX28RSC  OHX29RSC  \\\n",
       "8365      2.0      2.0      2.0  ...       b''       b''       b''       b''   \n",
       "\n",
       "      OHX30RSC  OHX31RSC  OHXRCAR  OHXRCARO  OHXRRES  OHXRRESO  \n",
       "8365       b''       b''      1.0       2.0      1.0       2.0  \n",
       "\n",
       "[1 rows x 171 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display one row\n",
    "pd_teeth_all.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e8bf9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regex to subset columns\n",
    "df_subset = pd_teeth_all.filter(regex='(cohort)|(SEQN)|(OHDDESTS)|(OHX)[0-9]{2}(TC)|(OHX)[0-9]{2}(CTC)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13ea738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off pandas chained assignment\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "# create a list that contains \"CTC\" in a column name\n",
    "ctc_col = [i for i in df_subset.columns if \"CTC\" in i]\n",
    "\n",
    "# create a list that has the categorical datatype columns\n",
    "categpry_cols = ['cohort','SEQN'] + ctc_col\n",
    "\n",
    "# create a list that has the integer datatype\n",
    "integer_cols = [i for i in df_subset.columns if i not in categpry_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2798b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the missing values to -1\n",
    "df_subset = df_subset.fillna(-1)\n",
    "\n",
    "# use for loops tp convert the data to category type and int32 type\n",
    "for col in categpry_cols:\n",
    "    df_subset[col] = df_subset[col].astype('category')\n",
    "for col in integer_cols:\n",
    "    df_subset[col] = df_subset[col].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "644b13e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SEQN        category\n",
       "OHDDESTS       int32\n",
       "OHX01TC        int32\n",
       "OHX02TC        int32\n",
       "OHX03TC        int32\n",
       "              ...   \n",
       "OHX28CTC    category\n",
       "OHX29CTC    category\n",
       "OHX30CTC    category\n",
       "OHX31CTC    category\n",
       "cohort      category\n",
       "Length: 63, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display datatypes\n",
    "df_subset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad23755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the column names\n",
    "df_subset.rename(columns={'SEQN': 'id'}, inplace=True)\n",
    "df_subset.head(1)\n",
    "\n",
    "# save it to pickle format\n",
    "df_subset.to_pickle(\"tooth.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35237c12",
   "metadata": {},
   "source": [
    "## C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8247a908",
   "metadata": {},
   "source": [
    "demographic dataset: 39156 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bad3462",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39156"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd6054",
   "metadata": {},
   "source": [
    "oral health and dentition dataset: 35909 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36a31912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35909"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d96a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d18cbae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cd3d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30710a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68b1b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6600cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdd9a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de90ccfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b61b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets\n",
    "dat_g = pd.read_sas(\"DEMO_G.XPT\")\n",
    "dat_h = pd.read_sas(\"DEMO_H.XPT\")\n",
    "dat_i = pd.read_sas(\"DEMO_I.XPT\")\n",
    "dat_j = pd.read_sas(\"DEMO_J.XPT\")\n",
    "\n",
    "# Add an additional column identifying to which cohort each case belongs\n",
    "dat_g['cohort'] = '11-12'\n",
    "dat_h['cohort'] = '13-14'\n",
    "dat_i['cohort'] = '15-16'\n",
    "dat_j['cohort'] = '17-18'\n",
    "\n",
    "# append multiple dataframes to one\n",
    "frames = [dat_g, dat_h,dat_i,dat_j]\n",
    "pd_all = pd.concat(frames)\n",
    "\n",
    "# subset columns\n",
    "df_all = pd_all[[\"cohort\",\"SEQN\",\"RIDAGEYR\",\"RIDRETH3\",\"DMDEDUC2\",\n",
    "               \"DMDMARTL\",\"RIDSTATR\",\"SDMVPSU\",\"SDMVSTRA\",\"WTMEC2YR\",\n",
    "               \"WTINT2YR\", \"RIAGENDR\"]]\n",
    "\n",
    "# turn off pandas chained assignment\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "# create a list that has the categorical datatype columns\n",
    "categpry_cols = ['cohort','SEQN','RIDRETH3','DMDEDUC2','DMDMARTL','RIAGENDR']\n",
    "\n",
    "# create a list that has the integer datatype columns\n",
    "integer_cols = ['RIDAGEYR','RIDSTATR','SDMVPSU','SDMVSTRA']\n",
    "\n",
    "# use for loops tp convert the data to category type and int32 type\n",
    "for col in categpry_cols:\n",
    "    df_all[col] = df_all[col].astype('category')\n",
    "for col in integer_cols:\n",
    "    df_all[col] = df_all[col].astype('int64')\n",
    "    \n",
    "df_all.rename(columns={'SEQN': 'id', \n",
    "                   'RIDAGEYR': 'age',\n",
    "                   'RIDRETH3':'race and ethnicity',\n",
    "                   'DMDEDUC2':'education',\n",
    "                   'DMDMARTL':'marital status',\n",
    "                   'RIAGENDR':'gender'\n",
    "                    }, inplace=True)\n",
    "\n",
    "# save dataframe to pickle format\n",
    "df_all.to_pickle(\"nhanes_gender.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b0169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f171ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
